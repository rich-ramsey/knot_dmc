
@Article{wagenmakers2008,
  title = {A Diffusion Model Account of Criterion Shifts in the Lexical Decision Task},
  author = {Eric-Jan Wagenmakers and Roger Ratcliff and Pablo Gomez and Gail McKoon},
  date = {2008-01-01},
  journaltitle = {Journal of Memory and Language},
  shortjournal = {Journal of Memory and Language},
  volume = {58},
  number = {1},
  pages = {140--159},
  issn = {0749-596X},
  doi = {10.1016/j.jml.2007.04.006},
  url = {https://www.sciencedirect.com/science/article/pii/S0749596X07000496},
  urldate = {2024-10-23},
  abstract = {Performance in the lexical decision task is highly dependent on decision criteria. These criteria can be influenced by speed versus accuracy instructions and word/nonword proportions. Experiment 1 showed that error responses speed up relative to correct responses under instructions to respond quickly. Experiment 2 showed that responses to less probable stimuli are slower and less accurate than responses to more probable stimuli. The data from both experiments support the diffusion model for lexical decision [Ratcliff, R., Gomez, P., \& McKoon, G. (2004a). A diffusion model account of the lexical decision task. Psychological Review, 111, 159–182]. At the same time, the data provide evidence against the popular deadline model for lexical decision. The deadline model assumes that “nonword” responses are given only after the “word” response has timed out—consequently, the deadline model cannot account for the data from experimental conditions in which “nonword” responses are systematically faster than “word” responses.},
  keywords = {Deadline model,Diffusion model,DRC,Lexical decision,MROM,Response criteria},
  file = {/Users/rramsey/Dropbox/docs/journals/cog_neuro/Wagenmakers_2008_Journal of Memory and LanguageA diffusion model account of criterion shifts in the lexical decision taskA diffusion model account of criterion shifts in the lexical decision task.pdf;/Users/rramsey/Zotero/storage/EJGC79LB/S0749596X07000496.html},
}
@Article{ratcliff2001,
  title = {The Effects of Aging on Reaction Time in a Signal Detection Task},
  author = {Roger Ratcliff and Anjali Thapar and Gail McKoon},
  date = {2001},
  journaltitle = {Psychology and Aging},
  volume = {16},
  number = {2},
  pages = {323--341},
  publisher = {American Psychological Association},
  location = {US},
  issn = {1939-1498},
  doi = {10.1037/0882-7974.16.2.323},
  abstract = {The effects of aging on response time are examined in 2 simple signal detection tasks with young and older subjects (age 60 years and older). Older subjects were generally slower than young subjects, and standard Brinley plot analyses of response times showed typical results: slopes greater than 1 and (mostly) negative intercepts. R. Ratcliff, D. Spieler, and G. McKoon (2000) showed that the slopes of Brinley plots measure the relative standard deviations of the distributions of response times for older versus young subjects. Applying R. Ratcliffs (1978) diffusion model to fit the response times, their distributions, and response accuracy, it was found that the larger spread in older subjects' response times and their slowness relative to young subjects comes from a 50-ms slowing of the nondecision components of response time and more from conservative settings of response criteria. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Adult Development,Age Differences,Aging,Reaction Time,Signal Detection (Perception)},
  file = {/Users/rramsey/Zotero/storage/CX2GURUM/2001-06652-013.html},
}

@Article{ratcliff2010,
  title = {Individual Differences, Aging, and {{IQ}} in Two-Choice Tasks},
  author = {Roger Ratcliff and Anjali Thapar and Gail McKoon},
  date = {2010-05-01},
  journaltitle = {Cognitive Psychology},
  shortjournal = {Cognitive Psychology},
  volume = {60},
  number = {3},
  pages = {127--157},
  issn = {0010-0285},
  doi = {10.1016/j.cogpsych.2009.09.001},
  url = {https://www.sciencedirect.com/science/article/pii/S0010028509000590},
  urldate = {2024-10-23},
  abstract = {The effects of aging and IQ on performance were examined in three two-choice tasks: numerosity discrimination, recognition memory, and lexical decision. The experimental data, accuracy, correct and error response times, and response time distributions, were well explained by Ratcliff’s (1978) diffusion model. The components of processing identified by the model were compared across levels of IQ (ranging from 83 to 146) and age (college students, 60–74, and 75–90year olds). Declines in performance with age were not significantly different for low compared to high IQ subjects. IQ but not age had large effects on the quality of the evidence that was obtained from a stimulus or memory, that is, the evidence upon which decisions were based. Applying the model to individual subjects, the components of processing identified by the model for individuals correlated across tasks. In addition, the model’s predictions and the data were examined for the “worst performance rule”, the finding that age and IQ have larger effects on slower responses than faster responses.},
  keywords = {Aging,Diffusion model,Individual differences,IQ,Reaction time},
  file = {/Users/rramsey/Dropbox/docs/journals/cog_neuro/Ratcliff_2010_Cognitive PsychologyIndividual differences, aging, and IQ in two-choice tasksIndividual differences, aging, and IQ in two-choice tasks.pdf;/Users/rramsey/Zotero/storage/I8XMH5FI/S0010028509000590.html},
}
@Article{ratcliff2008,
  title = {The {{Diffusion Decision Model}}: {{Theory}} and {{Data}} for {{Two-Choice Decision Tasks}}},
  shorttitle = {The {{Diffusion Decision Model}}},
  author = {Roger Ratcliff and Gail McKoon},
  date = {2008-04},
  journaltitle = {Neural Computation},
  volume = {20},
  number = {4},
  pages = {873--922},
  issn = {0899-7667},
  doi = {10.1162/neco.2008.12-06-420},
  url = {https://ieeexplore.ieee.org/abstract/document/6796810},
  urldate = {2024-10-23},
  abstract = {The diffusion decision model allows detailed explanations of behavior in two-choice discrimination tasks. In this article, the model is reviewed to show how it translates behavioral data—accuracy, mean response times, and response time distributions—into components of cognitive processing. Three experiments are used to illustrate experimental manipulations of three components: stimulus difficulty affects the quality of information on which a decision is based; instructions emphasizing either speed or accuracy affect the criterial amounts of information that a subject requires before initiating a response; and the relative proportions of the two stimuli affect biases in drift rate and starting point. The experiments also illustrate the strong constraints that ensure the model is empirically testable and potentially falsifiable. The broad range of applications of the model is also reviewed, including research in the domains of aging and neurophysiology.},
  eventtitle = {Neural {{Computation}}},
  file = {/Users/rramsey/Dropbox/docs/journals/cog_neuro/Ratcliff_McKoon_2008_The Diffusion Decision ModelThe Diffusion Decision Model.pdf;/Users/rramsey/Zotero/storage/LX9GWISS/6796810.html},
}
@Article{brown2008,
  title = {The Simplest Complete Model of Choice Response Time: {{Linear}} Ballistic Accumulation},
  shorttitle = {The Simplest Complete Model of Choice Response Time},
  author = {Scott D. Brown and Andrew Heathcote},
  date = {2008-11-01},
  journaltitle = {Cognitive Psychology},
  shortjournal = {Cognitive Psychology},
  volume = {57},
  number = {3},
  pages = {153--178},
  issn = {0010-0285},
  doi = {10.1016/j.cogpsych.2007.12.002},
  url = {https://www.sciencedirect.com/science/article/pii/S0010028507000722},
  urldate = {2022-07-08},
  abstract = {We propose a linear ballistic accumulator (LBA) model of decision making and reaction time. The LBA is simpler than other models of choice response time, with independent accumulators that race towards a common response threshold. Activity in the accumulators increases in a linear and deterministic manner. The simplicity of the model allows complete analytic solutions for choices between any number of alternatives. These solutions (and freely-available computer code) make the model easy to apply to both binary and multiple choice situations. Using data from five previously published experiments, we demonstrate that the LBA model successfully accommodates empirical phenomena from binary and multiple choice tasks that have proven difficult for other theoretical accounts. Our results are encouraging in a field beset by the tradeoff between complexity and completeness.},
  langid = {english},
  keywords = {Choice,Decision,Lexical decision,Mathematical models,Reaction time,Response time},
}
@Article{cross2012d,
  title = {Physical Experience Leads to Enhanced Object Perception in Parietal Cortex: {{Insights}} from Knot Tying},
  shorttitle = {Physical Experience Leads to Enhanced Object Perception in Parietal Cortex},
  author = {Emily S. Cross and Nichola Rice Cohen and Antonia F. de C. Hamilton and Richard Ramsey and George Wolford and Scott T. Grafton},
  date = {2012-12-01},
  journaltitle = {Neuropsychologia},
  shortjournal = {Neuropsychologia},
  volume = {50},
  number = {14},
  pages = {3207--3217},
  issn = {0028-3932},
  doi = {10.1016/j.neuropsychologia.2012.09.028},
  url = {https://www.sciencedirect.com/science/article/pii/S0028393212004009},
  urldate = {2024-07-02},
  abstract = {What does it mean to “know” what an object is? Viewing objects from different categories (e.g., tools vs. animals) engages distinct brain regions, but it is unclear whether these differences reflect object categories themselves or the tendency to interact differently with objects from different categories (grasping tools, not animals). Here we test how the brain constructs representations of objects that one learns to name or physically manipulate. Participants learned to name or tie different knots and brain activity was measured whilst performing a perceptual discrimination task with these knots before and after training. Activation in anterior intraparietal sulcus, a region involved in object manipulation, was specifically engaged when participants viewed knots they learned to tie. This suggests that object knowledge is linked to sensorimotor experience and its associated neural systems for object manipulation. Findings are consistent with a theory of embodiment in which there can be clear overlap in brain systems that support conceptual knowledge and control of object manipulation.},
  keywords = {Action learning,Neuroimaging,Object representation,Parietal,Perception},
  file = {/Users/rramsey/Dropbox/docs/journals/cog_neuro/Cross_2012_NeuropsychologiaPhysical experience leads to enhanced object perception in parietal cortexPhysical experience leads to enhanced object perception in parietal cortex.pdf;/Users/rramsey/Zotero/storage/HWAUPDXP/S0028393212004009.html},
}
@Article{wagenmakers2007,
  title = {An {{EZ-diffusion}} Model for Response Time and Accuracy},
  author = {Eric-Jan Wagenmakers and Han L. J. {Van Der Maas} and Raoul P. P. P. Grasman},
  date = {2007-02-01},
  journaltitle = {Psychonomic Bulletin \& Review},
  shortjournal = {Psychonomic Bulletin \& Review},
  volume = {14},
  number = {1},
  pages = {3--22},
  issn = {1531-5320},
  doi = {10.3758/BF03194023},
  url = {https://doi.org/10.3758/BF03194023},
  urldate = {2024-10-23},
  abstract = {The EZ-diffusion model for two-choice response time tasks takes mean response time, the variance of response time, and response accuracy as inputs. The model transforms these data via three simple equations to produce unique values for the quality of information, response conservativeness, and nondecision time. This transformation of observed data in terms of unobserved variables addresses the speed—accuracy trade-off and allows an unambiguous quantification of performance differences in two-choice response time tasks. The EZ-diffusion model can be applied to data-sparse situations to facilitate individual subject analysis. We studied the performance of the EZ-diffusion model in terms of parameter recovery and robustness against misspecification by using Monte Carlo simulations. The EZ model was also applied to a real-world data set.},
  langid = {english},
  keywords = {Boundary Separation,Diffusion Model,Drift Rate,Error Response,Parameter Recovery},
  file = {/Users/rramsey/Dropbox/docs/journals/cog_neuro/Wagenmakers_2007_Psychonomic Bulletin & ReviewAn EZ-diffusion model for response time and accuracyAn EZ-diffusion model for response time and accuracy.pdf},
}
@Article{ratcliff2002,
  title = {A Diffusion Model Account of Response Time and Accuracy in a Brightness Discrimination Task: {{Fitting}} Real Data and Failing to Fit Fake but Plausible Data},
  shorttitle = {A Diffusion Model Account of Response Time and Accuracy in a Brightness Discrimination Task},
  author = {Roger Ratcliff},
  date = {2002-06-01},
  journaltitle = {Psychonomic Bulletin \& Review},
  shortjournal = {Psychonomic Bulletin \& Review},
  volume = {9},
  number = {2},
  pages = {278--291},
  issn = {1531-5320},
  doi = {10.3758/BF03196283},
  url = {https://doi.org/10.3758/BF03196283},
  urldate = {2024-10-23},
  abstract = {A brightness discrimination experiment was performed to examine how subjects decide whether a patch of pixels is “bright” or “dark,” and stimulus duration, brightness, and speed versus accuracy instructions were manipulated. The diffusion model (Ratcliff, 1978) was fit to the data, and it accounted for all the dependent variables: mean correct and error response times, the shapes of response time distributions for correct and error responses, and accuracy values. Speed-accuracy manipulations affected only boundary separation (response criteria settings) in the model. Drift rate (the rate of accumulation of evidence) in the diffusion model, which represents stimulus quality, increased as a function of stimulus duration and stimulus brightness but asymptoted as stimulus duration increased from 100 to 150 msec. To address the argument that the diffusion model can fit any pattern of data, simulated patterns of plausible data are presented that the model cannot fit.},
  langid = {english},
  keywords = {Boundary Separation,Diffusion Model,Drift Rate,Error Response,Stimulus Duration},
  file = {/Users/rramsey/Dropbox/docs/journals/cog_neuro/Ratcliff_2002_Psychonomic Bulletin & ReviewA diffusion model account of response time and accuracy in a brightnessA diffusion model account of response time and accuracy in a brightness.pdf},
}
@Article{donkin2009,
  title = {Getting More from Accuracy and Response Time Data: {{Methods}} for Fitting the Linear Ballistic Accumulator},
  shorttitle = {Getting More from Accuracy and Response Time Data},
  author = {Chris Donkin and Lee Averell and Scott Brown and Andrew Heathcote},
  date = {2009-11-01},
  journaltitle = {Behavior Research Methods},
  shortjournal = {Behavior Research Methods},
  volume = {41},
  number = {4},
  pages = {1095--1110},
  issn = {1554-3528},
  doi = {10.3758/BRM.41.4.1095},
  url = {https://doi.org/10.3758/BRM.41.4.1095},
  urldate = {2022-07-08},
  abstract = {Cognitive models of the decision process provide greater insight into response time and accuracy than do standard ANOVA techniques. However, such models can be mathematically and computationally difficult to apply. We provide instructions and computer code for three methods for estimating the parameters of the linear ballistic accumulator (LBA), a new and computationally tractable model of decisions between two or more choices. These methods—a Microsoft Excel worksheet, scripts for the statistical program R, and code for implementation of the LBA into the Bayesian sampling software WinBUGS—vary in their flexibility and user accessibility. We also provide scripts in R that produce a graphical summary of the data and model predictions. In a simulation study, we explored the effect of sample size on parameter recovery for each method. The materials discussed in this article may be downloaded as a supplement from http://brm.psychonomic-journals.org/content/supplemental.},
  langid = {english},
  keywords = {Drift Rate,Error Response,Graphical Summary,Markov Chain Monte Carlo,Posterior Distribution},
}
@InCollection{lewandowsky2018,
  title = {Computational {{Modeling}} in {{Cognition}} and {{Cognitive Neuroscience}}},
  booktitle = {Stevens' {{Handbook}} of {{Experimental Psychology}} and {{Cognitive Neuroscience}}},
  author = {Stephan Lewandowsky and Klaus Oberauer},
  date = {2018},
  pages = {1--35},
  publisher = {John Wiley \& Sons, Ltd},
  doi = {10.1002/9781119170174.epcn501},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781119170174.epcn501},
  urldate = {2024-10-23},
  abstract = {We survey the utility and function of mathematical and computational models in cognitive science by emphasizing their role as a reasoning aid for researchers. Unlike purely verbal theorizing, the rigor of computational models forces researchers to understand the implications of theoretical constructs. We review four types of models: (1) Descriptive models, which replace the raw data with parameter estimates but are not constrained by any prior theoretical assumptions. (2) Measurement models, which are also fitted to individuals or experimental conditions by unconstrained parameter estimation, but that embody strong structural assumptions that constrain the way in which dependent variables in an experiment are related. (3) Explanatory models, which go beyond measurement models by postulating not only the processes that operate in a single condition in an experiment but also explain how and why experimental conditions differ from each other. (4) Finally, cognitive architectures. which seek to explain a broad range of cognitive processes and activities at a higher level of (often symbolic) abstraction. For all types of model, we show how they can be related to the neural underpinnings of cognition.},
  isbn = {978-1-119-17017-4},
  langid = {english},
  keywords = {cognition,decision making,memory,modeling},
  file = {/Users/rramsey/Dropbox/docs/journals/cog_neuro/Lewandowsky_Oberauer_2018_Computational Modeling in Cognition and Cognitive NeuroscienceComputational Modeling in Cognition and Cognitive Neuroscience.pdf;/Users/rramsey/Zotero/storage/2TMVDT4D/9781119170174.html},
}
@Article{evans2019,
  title = {A Method, Framework, and Tutorial for Efficiently Simulating Models of Decision-Making},
  author = {Nathan J. Evans},
  date = {2019-10-01},
  journaltitle = {Behavior Research Methods},
  shortjournal = {Behav Res},
  volume = {51},
  number = {5},
  pages = {2390--2404},
  issn = {1554-3528},
  doi = {10.3758/s13428-019-01219-z},
  url = {https://doi.org/10.3758/s13428-019-01219-z},
  urldate = {2024-01-26},
  abstract = {Evidence accumulation models (EAMs) have become the dominant models of rapid decision-making. Several variants of these models have been proposed, ranging from the simple linear ballistic accumulator (LBA) to the more complex leaky-competing accumulator (LCA), and further extensions that include time-varying rates of evidence accumulation or decision thresholds. Although applications of the simpler variants have been widespread, applications of the more complex models have been fewer, largely due to their intractable likelihood function and the computational cost of mass simulation. Here, I present a framework for efficiently fitting complex EAMs, which uses a new, efficient method of simulating these models. I find that the majority of simulation time is taken up by random number generation (RNG) from the normal distribution, needed for the stochastic noise of the differential equation. To reduce this inefficiency, I propose using the well-known concept within computer science of “look-up tables” (LUTs) as an approximation to the inverse cumulative density function (iCDF) method of RNG, which I call “LUT-iCDF”. I show that when using an appropriately sized LUT, simulations using LUT-iCDF closely match those from the standard RNG method in R. My framework, which I provide a detailed tutorial on how to implement, includes C code for 12 different variants of EAMs using the LUT-iCDF method, and should make the implementation of complex EAMs easier and faster.},
  langid = {english},
  keywords = {Decision-making,Evidence accumulation models,Probability density approximation,Random number generation},
  file = {/Users/rramsey/Dropbox/docs/journals/cog_neuro/Evans_2019_Behav ResA method, framework, and tutorial for efficiently simulating models ofA method, framework, and tutorial for efficiently simulating models of.pdf},
}

@Article{evans2020,
  title = {Evidence {{Accumulation Models}}: {{Current Limitations}} and {{Future Directions}}},
  shorttitle = {Evidence {{Accumulation Models}}},
  author = {Nathan J. Evans and Eric-Jan Wagenmakers},
  date = {2020-04-01},
  journaltitle = {The Quantitative Methods for Psychology},
  shortjournal = {TQMP},
  volume = {16},
  number = {2},
  pages = {73--90},
  issn = {2292-1354},
  doi = {10.20982/tqmp.16.2.p073},
  url = {http://www.tqmp.org/RegularArticles/vol16-2/p073},
  urldate = {2024-10-23},
  file = {/Users/rramsey/Dropbox/docs/journals/cog_neuro/Evans_Wagenmakers_2020_TQMPEvidence Accumulation ModelsEvidence Accumulation Models.pdf},
}
@Article{ratcliff1998,
  title = {Modeling {{Response Times}} for {{Two-Choice Decisions}}},
  author = {Roger Ratcliff and Jeffrey N. Rouder},
  date = {1998-09-01},
  journaltitle = {Psychological Science},
  shortjournal = {Psychol Sci},
  volume = {9},
  number = {5},
  pages = {347--356},
  publisher = {SAGE Publications Inc},
  issn = {0956-7976},
  doi = {10.1111/1467-9280.00067},
  url = {https://doi.org/10.1111/1467-9280.00067},
  urldate = {2024-10-23},
  abstract = {The diffusion model for two-choice real-time decisions is applied to four psychophysical tasks. The model reveals how stimulus information guides decisions and shows how the information is processed through time to yield sometimes correct and sometimes incorrect decisions. Rapid two-choice decisions yield multiple empirical measures: response times for correct and error responses, the probabilities of correct and error responses, and a variety of interactions between accuracy and response time that depend on instructions and task difficulty. The diffusion model can explain all these aspects of the data for the four experiments we present. The model correctly accounts for error response times, something previous models have failed to do. Variability within the decision process explains how errors are made, and variability across trials correctly predicts when errors are faster than correct responses and when they are slower.},
  langid = {english},
  file = {/Users/rramsey/Dropbox/docs/journals/cog_neuro/Ratcliff_Rouder_1998_Psychol SciModeling Response Times for Two-Choice DecisionsModeling Response Times for Two-Choice Decisions.pdf},
}@Article{dutilh2019,
  title = {The Quality of Response Time Data Inference: A Blinded, Collaborative Assessment of the Validity of Cognitive Models},
  shorttitle = {The Quality of Response Time Data Inference},
  author = {Gilles Dutilh and Jeffrey Annis and Scott D. Brown and Peter Cassey and Nathan J. Evans and Raoul P. P. P. Grasman and Guy E. Hawkins and Andrew Heathcote and William R. Holmes and Angelos-Miltiadis Krypotos and Colin N. Kupitz and F{\a'a}bio P. Leite and Veronika Lerche and Yi-Shin Lin and Gordon D. Logan and Thomas J. Palmeri and Jeffrey J. Starns and Jennifer S. Trueblood and Leendert {van Maanen} and Don {van Ravenzwaaij} and Joachim Vandekerckhove and Ingmar Visser and Andreas Voss and Corey N. White and Thomas V. Wiecki and J{\"o}rg Rieskamp and Chris Donkin},
  date = {2019-08-01},
  journaltitle = {Psychonomic Bulletin \& Review},
  shortjournal = {Psychon Bull Rev},
  volume = {26},
  number = {4},
  pages = {1051--1069},
  issn = {1531-5320},
  doi = {10.3758/s13423-017-1417-2},
  url = {https://doi.org/10.3758/s13423-017-1417-2},
  urldate = {2024-10-23},
  abstract = {Most data analyses rely on models. To complement statistical models, psychologists have developed cognitive models, which translate observed variables into psychologically interesting constructs. Response time models, in particular, assume that response time and accuracy are the observed expression of latent variables including 1) ease of processing, 2) response caution, 3) response bias, and 4) non-decision time. Inferences about these psychological factors, hinge upon the validity of the models’ parameters. Here, we use a blinded, collaborative approach to assess the validity of such model-based inferences. Seventeen teams of researchers analyzed the same 14 data sets. In each of these two-condition data sets, we manipulated properties of participants’ behavior in a two-alternative forced choice task. The contributing teams were blind to the manipulations, and had to infer what aspect of behavior was changed using their method of choice. The contributors chose to employ a variety of models, estimation methods, and inference procedures. Our results show that, although conclusions were similar across different methods, these "modeler’s degrees of freedom" did affect their inferences. Interestingly, many of the simpler approaches yielded as robust and accurate inferences as the more complex methods. We recommend that, in general, cognitive models become a typical analysis tool for response time data. In particular, we argue that the simpler models and procedures are sufficient for standard experimental designs. We finish by outlining situations in which more complicated models and methods may be necessary, and discuss potential pitfalls when interpreting the output from response time models.},
  langid = {english},
  keywords = {Cognitive modeling,Diffusion Model,LBA,Response Times,Validity},
  file = {/Users/rramsey/Dropbox/docs/journals/cog_neuro/Dutilh_2019_Psychon Bull RevThe Quality of Response Time Data InferenceThe Quality of Response Time Data Inference.pdf},
}
@Article{salthouse1996,
  title = {The Processing-Speed Theory of Adult Age Differences in Cognition},
  author = {Timothy A. Salthouse},
  date = {1996},
  journaltitle = {Psychological Review},
  volume = {103},
  number = {3},
  pages = {403--428},
  publisher = {American Psychological Association},
  location = {US},
  issn = {1939-1471},
  doi = {10.1037/0033-295X.103.3.403},
  abstract = {A theory is proposed to account for some of the age-related differences reported in measures of Type A or fluid cognition. The central hypothesis in the theory is that increased age in adulthood is associated with a decrease in the speed with which many processing operations can be executed and that this reduction in speed leads to impairments in cognitive functioning because of what are termed the limited time mechanism and the simultaneity mechanism. That is, cognitive performance is degraded when processing is slow because relevant operations cannot be successfully executed(limited time) and because the products of early  processing may no longer be available when later processing is complete ( simultaneity ) . Several types of evidence, such as the discovery of considerable shared age-related variance across various measures of speed and large attenuation of the age-related influences on cognitive measures after statistical control of measures of speed, are consistent with this theory. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Age Differences,Aging,Cognitive Processes,Reaction Time,Theories},
  file = {/Users/rramsey/Zotero/storage/YK9WK95P/1996-01780-001.html},
}
@Article{white2022,
  title = {On the {{Need}} to {{Improve}} the {{Way Individual Differences}} in {{Cognitive Function Are Measured With Reaction Time Tasks}}},
  author = {Corey N. White and Kiah N. Kitchen},
  date = {2022-06-01},
  journaltitle = {Current Directions in Psychological Science},
  shortjournal = {Curr Dir Psychol Sci},
  volume = {31},
  number = {3},
  pages = {223--230},
  publisher = {SAGE Publications Inc},
  issn = {0963-7214},
  doi = {10.1177/09637214221077060},
  url = {https://doi.org/10.1177/09637214221077060},
  urldate = {2024-10-23},
  abstract = {The measurement of individual differences in specific cognitive functions has been an important area of study for decades. Often the goal of such studies is to determine whether there are cognitive deficits or enhancements associated with, for example, a specific population, psychological disorder, health status, or age group. The inherent difficulty, however, is that most cognitive functions are not directly observable, so researchers rely on indirect measures to infer an individual’s functioning. One of the most common approaches is to use a task that is designed to tap into a specific function and to use behavioral measures, such as reaction times (RTs), to assess performance on that task. Although this approach is widespread, it unfortunately is subject to a problem of reverse inference: Differences in a given cognitive function can be manifest as differences in RTs, but that does not guarantee that differences in RTs imply differences in that cognitive function. We illustrate this inference problem with data from a study on aging and lexical processing, highlighting how RTs can lead to erroneous conclusions about processing. Then we discuss how employing choice-RT models to analyze data can improve inference and highlight practical approaches to improving the models and incorporating them into one’s analysis pipeline.},
  langid = {english},
}
@Article{simons2016,
  title = {Do “{{Brain-Training}}” {{Programs Work}}?},
  author = {Daniel J. Simons and Walter R. Boot and Neil Charness and Susan E. Gathercole and Christopher F. Chabris and David Z. Hambrick and Elizabeth A. L. Stine-Morrow},
  date = {2016-10-01},
  journaltitle = {Psychological Science in the Public Interest},
  shortjournal = {Psychol Sci Public Interest},
  volume = {17},
  number = {3},
  pages = {103--186},
  publisher = {SAGE Publications Inc},
  issn = {1529-1006},
  doi = {10.1177/1529100616661983},
  url = {https://doi.org/10.1177/1529100616661983},
  urldate = {2024-10-23},
  abstract = {In 2014, two groups of scientists published open letters on the efficacy of brain-training interventions, or “brain games,” for improving cognition. The first letter, a consensus statement from an international group of more than 70 scientists, claimed that brain games do not provide a scientifically grounded way to improve cognitive functioning or to stave off cognitive decline. Several months later, an international group of 133 scientists and practitioners countered that the literature is replete with demonstrations of the benefits of brain training for a wide variety of cognitive and everyday activities. How could two teams of scientists examine the same literature and come to conflicting “consensus” views about the effectiveness of brain training? In part, the disagreement might result from different standards used when evaluating the evidence. To date, the field has lacked a comprehensive review of the brain-training literature, one that examines both the quantity and the quality of the evidence according to a well-defined set of best practices. This article provides such a review, focusing exclusively on the use of cognitive tasks or games as a means to enhance performance on other tasks. We specify and justify a set of best practices for such brain-training interventions and then use those standards to evaluate all of the published peer-reviewed intervention studies cited on the websites of leading brain-training companies listed on Cognitive Training Data (www.cognitivetrainingdata.org), the site hosting the open letter from brain-training proponents. These citations presumably represent the evidence that best supports the claims of effectiveness. Based on this examination, we find extensive evidence that brain-training interventions improve performance on the trained tasks, less evidence that such interventions improve performance on closely related tasks, and little evidence that training enhances performance on distantly related tasks or that training improves everyday cognitive performance. We also find that many of the published intervention studies had major shortcomings in design or analysis that preclude definitive conclusions about the efficacy of training, and that none of the cited studies conformed to all of the best practices we identify as essential to drawing clear conclusions about the benefits of brain training for everyday activities. We conclude with detailed recommendations for scientists, funding agencies, and policymakers that, if adopted, would lead to better evidence regarding the efficacy of brain-training interventions.},
  langid = {english},
}

@Article{vonbastian2022,
  title = {Mechanisms Underlying Training-Induced Cognitive Change},
  author = {Claudia C. {von Bastian} and Sylvie Belleville and Robert C. Udale and Alice Reinhartz and Mehdi Essounni and Tilo Strobach},
  date = {2022-01},
  journaltitle = {Nature Reviews Psychology},
  shortjournal = {Nat Rev Psychol},
  volume = {1},
  number = {1},
  pages = {30--41},
  publisher = {Nature Publishing Group},
  issn = {2731-0574},
  doi = {10.1038/s44159-021-00001-3},
  url = {https://www.nature.com/articles/s44159-021-00001-3},
  urldate = {2024-10-23},
  abstract = {The prospect of enhancing cognition through behavioural training interventions, for example, the repetitive practice of cognitive tasks or metacognitive strategy instruction, has seen a surge in popularity over the past 20\,years. Although overwhelming evidence demonstrates that such training interventions increase performance in the trained tasks, controversy remains over whether these benefits transfer to other tasks and abilities beyond the trained context. In this Review, we provide an overview of the effectiveness of cognitive training to induce transfer to untrained tasks, with a particular focus on the theoretical mechanisms that have been proposed to underlie training and transfer effects. We highlight that there is relatively little evidence that training enhances cognitive capacity, that is, the overall cognitive resources available to an individual. By contrast, substantial evidence supports training-induced improvements in cognitive efficiency, that is, optimized performance within existing cognitive capacity limits. We conclude that shifting research towards identifying the cognitive mechanisms underlying gains in cognitive efficiency offers a fruitful avenue for developing effective cognitive training interventions. However, to advance our understanding of human cognition and cognitive plasticity we must strive to develop and refine theories that generate testable hypotheses.},
  langid = {english},
  keywords = {Attention,Behavioural methods,Cognitive control,Learning and memory,Working memory},
  file = {/Users/rramsey/Dropbox/docs/journals/cog_neuro/von Bastian_2022_Nat Rev PsycholMechanisms underlying training-induced cognitive changeMechanisms underlying training-induced cognitive change.pdf},
}
@Article{dutilh2009,
  title = {A Diffusion Model Decomposition of the Practice Effect},
  author = {Gilles Dutilh and Joachim Vandekerckhove and Francis Tuerlinckx and Eric-Jan Wagenmakers},
  date = {2009-12-01},
  journaltitle = {Psychonomic Bulletin \& Review},
  shortjournal = {Psychonomic Bulletin \& Review},
  volume = {16},
  number = {6},
  pages = {1026--1036},
  issn = {1531-5320},
  doi = {10.3758/16.6.1026},
  url = {https://doi.org/10.3758/16.6.1026},
  urldate = {2024-10-23},
  abstract = {When people repeatedly perform the same cognitive task, their mean response times (RTs) invariably decrease. The mathematical function that best describes this decrease has been the subject of intense debate. Here, we seek a deeper understanding of the practice effect by simultaneously taking into account the changes in accuracy and in RT distributions with practice, both for correct and error responses. To this end, we used the Ratcliff diffusion model, a successful model of two-choice RTs that decomposes the effect of practice into its constituent psychological processes. Analyses of data from a 10,000-trial lexical decision task demonstrate that practice not only affects the speed of information processing, but also response caution, response bias, and peripheral processing time. We conclude that the practice effect consists of multiple subcomponents, and that it may be hazardous to abstract the interactive combination of these subcomponents in terms of a single output measure such as mean RT for correct responses. Supplemental materials may be downloaded from http://pbr .psychonomic-journals.org/content/supplemental.},
  langid = {english},
  keywords = {Diffusion Model,Drift Rate,Lexical Decision Task,Posterior Distribution,Practice Effect},
  file = {/Users/rramsey/Dropbox/docs/journals/cog_neuro/Dutilh_2009_Psychonomic Bulletin & ReviewA diffusion model decomposition of the practice effectA diffusion model decomposition of the practice effect.pdf},
}

@Article{dutilh2011,
  title = {Task-{{Related Versus Stimulus-Specific Practice}}},
  author = {Gilles Dutilh and Angelos-Miltiadis Krypotos and Eric-Jan Wagenmakers},
  date = {2011-01},
  journaltitle = {Experimental Psychology},
  volume = {58},
  number = {6},
  pages = {434--442},
  publisher = {Hogrefe Publishing},
  issn = {1618-3169},
  doi = {10.1027/1618-3169/a000111},
  url = {https://econtent.hogrefe.com/doi/abs/10.1027/1618-3169/a000111},
  urldate = {2024-10-23},
  abstract = {When people repeatedly practice the same cognitive task, their response times (RT) invariably decrease. Dutilh, Vandekerckhove, Tuerlinckx, and Wagenmakers (2009) argued that the traditional focus on how mean RT decreases with practice offers limited insight; their diffusion model analysis showed that the effect of practice is multifaceted, involving an increase in rate of information processing, a decrease in response caution, adjusted response bias, and, unexpectedly, a strong decrease in nondecision time. In this study, we aim to further disentangle these effects into stimulus-specific and task-related components. The data of a transfer experiment, in which repeatedly presented sets and new sets of stimuli were alternated, show that the practice effects on both speed of information processing and time needed for peripheral processing are partly task-related and partly stimulus-specific. The effects on response caution and response bias appear to be task-related. This diffusion model decomposition provides a perspective on practice that is more detailed and more informative than the traditional analysis of mean RT.},
  keywords = {classification,diffusion model,practice,response times},
}

@Article{reinhartz2023,
  title = {Mechanisms of {{Training-Related Change}} in {{Processing Speed}}: {{A Drift-Diffusion Model Approach}}},
  shorttitle = {Mechanisms of {{Training-Related Change}} in {{Processing Speed}}},
  author = {Alice Reinhartz and Tilo Strobach and Thomas Jacobsen and Claudia C. {von Bastian}},
  date = {2023-08-18},
  journaltitle = {Journal of Cognition},
  volume = {6},
  number = {1},
  eprint = {37600217},
  eprinttype = {pmid},
  pages = {46},
  doi = {10.5334/joc.310},
  url = {https://pmc.ncbi.nlm.nih.gov/articles/PMC10437139/},
  urldate = {2024-10-23},
  abstract = {Processing speed is a crucial ability that changes over the course of the lifespan. Training interventions on processing speed have shown promising effects and have been associated with improved cognitive functioning. While training-related changes ...},
  langid = {english},
  file = {/Users/rramsey/Dropbox/docs/journals/cog_neuro/Reinhartz_2023_Mechanisms of Training-Related Change in Processing SpeedMechanisms of Training-Related Change in Processing Speed.pdf},
}
@Article{liu2012,
  title = {Accounting for Speed–Accuracy Tradeoff in Perceptual Learning},
  author = {Charles C. Liu and Takeo Watanabe},
  date = {2012-05-15},
  journaltitle = {Vision Research},
  shortjournal = {Vision Research},
  series = {Perceptual {{Learning}} - Mechanisms and Manifestations},
  volume = {61},
  pages = {107--114},
  issn = {0042-6989},
  doi = {10.1016/j.visres.2011.09.007},
  url = {https://www.sciencedirect.com/science/article/pii/S0042698911003373},
  urldate = {2024-10-23},
  abstract = {In the perceptual learning (PL) literature, researchers typically focus on improvements in accuracy, such as d′. In contrast, researchers who investigate the practice of cognitive skills focus on improvements in response times (RT). Here, we argue for the importance of accounting for both accuracy and RT in PL experiments, due to the phenomenon of speed–accuracy tradeoff (SAT): at a given level of discriminability, faster responses tend to produce more errors. A formal model of the decision process, such as the diffusion model, can explain the SAT. In this model, a parameter known as the drift rate represents the perceptual strength of the stimulus, where higher drift rates lead to more accurate and faster responses. We applied the diffusion model to analyze responses from a yes–no coherent motion detection task. The results indicate that observers do not use a fixed threshold for evidence accumulation, so changes in the observed accuracy may not provide the most appropriate estimate of learning. Instead, our results suggest that SAT can be accounted for by a modeling approach, and that drift rates offer a promising index of PL.},
  keywords = {Perceptual learning,Response times,Signal detection theory,Speed–accuracy tradeoff},
  file = {/Users/rramsey/Dropbox/docs/journals/cog_neuro/Liu_Watanabe_2012_Vision ResearchAccounting for speed–accuracy tradeoff in perceptual learningAccounting for speed–accuracy tradeoff in perceptual learning.pdf;/Users/rramsey/Zotero/storage/B7V7VCQD/S0042698911003373.html},
}

@Article{ratcliff2006,
  title = {Aging, Practice, and Perceptual Tasks: {{A}} Diffusion Model Analysis},
  shorttitle = {Aging, Practice, and Perceptual Tasks},
  author = {Roger Ratcliff and Anjali Thapar and Gail McKoon},
  date = {2006},
  journaltitle = {Psychology and Aging},
  volume = {21},
  number = {2},
  pages = {353--371},
  publisher = {American Psychological Association},
  location = {US},
  issn = {1939-1498},
  doi = {10.1037/0882-7974.21.2.353},
  abstract = {Practice effects were examined in a masked letter discrimination task and a masked brightness discrimination task for college-age and 60- to 75-year-old subjects. The diffusion model (Ratcliff, 1978) was fit to the response time and accuracy data and used to extract estimates of components of processing from the data. Relative to young subjects, the older subjects began the experiments with slower and less accurate performance; however, across sessions their accuracy improved because the quality of the information on which their decisions were based improved, and this, along with reduced decision criteria, led to shorter response times. For the brightness, but not the letter, discrimination task, the older subjects' performance matched that of the younger group by the end of 4 sessions, except that their nondecision components of processing were slightly slower. These analyses illustrate how a well-specified model can provide a unified view of multiple aspects of data that are often interpreted separately. (PsycInfo Database Record (c) 2023 APA, all rights reserved)},
  keywords = {Age Differences,Aging,College Students,Perceptual Learning,Practice,Reaction Time},
  file = {/Users/rramsey/Dropbox/docs/journals/cog_neuro/Ratcliff_2006_Aging, practice, and perceptual tasksAging, practice, and perceptual tasks.pdf;/Users/rramsey/Zotero/storage/R7936PZP/2006-07381-013.html},
}

@Article{zhang2014,
  title = {Dissociable Mechanisms of Speed-Accuracy Tradeoff during Visual Perceptual Learning Are Revealed by a Hierarchical Drift-Diffusion Model},
  author = {Jiaxiang Zhang and James B. Rowe},
  date = {2014-04-09},
  journaltitle = {Frontiers in Neuroscience},
  shortjournal = {Front. Neurosci.},
  volume = {8},
  publisher = {Frontiers},
  issn = {1662-453X},
  doi = {10.3389/fnins.2014.00069},
  url = {https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2014.00069/full},
  urldate = {2024-10-23},
  abstract = {{$<$}p{$>$}Two phenomena are commonly observed in decision-making. First, there is a speed-accuracy tradeoff (SAT) such that decisions are slower and more accurate when instructions emphasize accuracy over speed, and {$<$}italic{$>$}vice versa{$<$}/italic{$>$}. Second, decision performance improves with practice, as a task is learnt. The SAT and learning effects have been explained under a well-established evidence-accumulation framework for decision-making, which suggests that evidence supporting each choice is accumulated over time, and a decision is committed to when the accumulated evidence reaches a decision boundary. This framework suggests that changing the decision boundary creates the tradeoff between decision speed and accuracy, while increasing the rate of accumulation leads to more accurate and faster decisions after learning. However, recent studies challenged the view that SAT and learning are associated with changes in distinct, single decision parameters. Further, the influence of speed-accuracy instructions over the course of learning remains largely unknown. Here, we used a hierarchical drift-diffusion model to examine the SAT during learning of a coherent motion discrimination task across multiple training sessions, and a transfer test session. The influence of speed-accuracy instructions was robust over training and generalized across untrained stimulus features. Emphasizing decision accuracy rather than speed was associated with increased boundary separation, drift rate and non-decision time at the beginning of training. However, after training, an emphasis on decision accuracy was only associated with increased boundary separation. In addition, faster and more accurate decisions after learning were due to a gradual decrease in boundary separation and an increase in drift rate. The results suggest that speed-accuracy instructions and learning differentially shape decision-making processes at different time scales.{$<$}/p{$>$}},
  langid = {english},
  keywords = {Bayesian parameter estimation,Drift diffusion model,motion discrimination,Perceptual Learning,Speed-accuracy tradeoff},
  file = {/Users/rramsey/Dropbox/docs/journals/cog_neuro/Zhang_Rowe_2014_Front. Neurosci.Dissociable mechanisms of speed-accuracy tradeoff during visual perceptualDissociable mechanisms of speed-accuracy tradeoff during visual perceptual.pdf},
}
@Article{strobach2013,
  title = {Effects of Extensive Dual-Task Practice on Processing Stages in Simultaneous Choice Tasks},
  author = {Tilo Strobach and Roman Liepelt and Harold Pashler and Peter A. Frensch and Torsten Schubert},
  date = {2013-07-01},
  journaltitle = {Attention, Perception, \& Psychophysics},
  shortjournal = {Atten Percept Psychophys},
  volume = {75},
  number = {5},
  pages = {900--920},
  issn = {1943-393X},
  doi = {10.3758/s13414-013-0451-z},
  url = {https://doi.org/10.3758/s13414-013-0451-z},
  urldate = {2024-10-23},
  abstract = {Schumacher et al. Psychological Science 12:101–108, (2001) demonstrated the elimination of most dual-task costs (“perfect time-sharing”) after extensive dual-task practice of a visual and an auditory task in combination. For the present research, we used a transfer methodology to examine this practice effect in more detail, asking what task-processing stages were sped up by this dual-task practice. Such research will be essential to specify mechanisms associated with the practice-related elimination of dual-task costs. In three experiments, we introduced postpractice transfer probes focusing on the perception, central response-selection, and final motor-response stages. The results indicated that the major change achieved by dual-task practice was a speed-up in the central response-selection stages of both tasks. Additionally, perceptual-stage shortening of the auditory task was found to contribute to the improvements in time-sharing. For a better understanding of such time-sharing, we discuss the contributions of the present findings in relation to models of practiced dual-task performance.},
  langid = {english},
  keywords = {Dual-task performance,Practice effects,Processing stage shortening},
  file = {/Users/rramsey/Dropbox/docs/journals/cog_neuro/Strobach_2013_Atten Percept PsychophysEffects of extensive dual-task practice on processing stages in simultaneousEffects of extensive dual-task practice on processing stages in simultaneous.pdf},
}
@Article{cochrane2023,
  title = {Multiple Timescales of Learning Indicated by Changes in Evidence-Accumulation Processes during Perceptual Decision-Making},
  author = {Aaron Cochrane and Chris R. Sims and Vikranth R. Bejjanki and C. Shawn Green and Daphne Bavelier},
  date = {2023-06-08},
  journaltitle = {npj Science of Learning},
  shortjournal = {npj Sci. Learn.},
  volume = {8},
  number = {1},
  pages = {1--10},
  publisher = {Nature Publishing Group},
  issn = {2056-7936},
  doi = {10.1038/s41539-023-00168-9},
  url = {https://www.nature.com/articles/s41539-023-00168-9},
  urldate = {2024-10-23},
  abstract = {Evidence accumulation models have enabled strong advances in our understanding of decision-making, yet their application to examining learning has not been common. Using data from participants completing a dynamic random dot-motion direction discrimination task across four days, we characterized alterations in two components of perceptual decision-making (Drift Diffusion Model drift rate and response boundary). Continuous-time learning models were applied to characterize trajectories of performance change, with different models allowing for varying dynamics. The best-fitting model included drift rate changing as a continuous, exponential function of cumulative trial number. In contrast, response boundary changed within each daily session, but in an independent manner across daily sessions. Our results highlight two different processes underlying the pattern of behavior observed across the entire learning trajectory, one involving a continuous tuning of perceptual sensitivity, and another more variable process describing participants’ threshold of when enough evidence is present to act.},
  langid = {english},
  keywords = {Decision making,Human behaviour,Motion detection},
  file = {/Users/rramsey/Dropbox/docs/journals/cog_neuro/Cochrane_2023_npj Sci. Learn.Multiple timescales of learning indicated by changes in evidence-accumulationMultiple timescales of learning indicated by changes in evidence-accumulation.pdf},
}
@Report{simmons2012,
  type = {SSRN Scholarly Paper},
  title = {A 21 {{Word Solution}}},
  author = {Joseph P. Simmons and Leif D. Nelson and Uri Simonsohn},
  date = {2012-10-14},
  number = {2160588},
  institution = {Social Science Research Network},
  location = {Rochester, NY},
  doi = {10.2139/ssrn.2160588},
  url = {https://papers.ssrn.com/abstract=2160588},
  urldate = {2022-04-29},
  abstract = {One year after publishing "False-Positive Psychology," we propose a simple implementation of disclosure that requires but 21 words to achieve full transparency. This article is written in a casual tone. It includes phone-taken pictures of milk-jars and references to ice-cream and sardines.},
  langid = {english},
  keywords = {disclosure,false positive,p-hacking,psycholog,transparency},
  file = {/Users/rramsey/Zotero/storage/FR2WXRBA/papers.html},
}
@Article{weisberg2007,
  title = {A {{Neural System}} for {{Learning}} about {{Object Function}}},
  author = {Jill Weisberg and Miranda {van Turennout} and Alex Martin},
  date = {2007-03-01},
  journaltitle = {Cerebral Cortex},
  volume = {17},
  number = {3},
  pages = {513--521},
  doi = {10.1093/cercor/bhj176},
  url = {http://cercor.oxfordjournals.org/content/17/3/513.abstract},
  abstract = {Does our ability to visually identify everyday objects rely solely on access to information about their appearance or on a more distributed representation incorporating other object properties? Using functional magnetic resonance imaging, we addressed this question by having subjects visually match pictures of novel objects before and after extensive training to use these objects to perform specific tool-like tasks. After training, neural activity emerged in regions associated with the motion (left middle temporal gyrus) and manipulation (left intraparietal sulcus and premotor cortex) of common tools, whereas activity became more focal and selective in regions representing their visual appearance (fusiform gyrus). These findings indicate that this distributed network is automatically engaged in support of object identification. Moreover, the regions included in this network mirror those active when subjects retrieve information about tools and their properties, suggesting that, as a result of training, these previously novel objects have attained the conceptual status of ‚Äútools.‚Äù},
}
@Article{heathcote2019,
  title = {Dynamic Models of Choice},
  author = {Andrew Heathcote and Yi-Shin Lin and Angus Reynolds and Luke Strickland and Matthew Gretton and Dora Matzke},
  date = {2019-04-01},
  journaltitle = {Behavior Research Methods},
  shortjournal = {Behav Res},
  volume = {51},
  number = {2},
  pages = {961--985},
  issn = {1554-3528},
  doi = {10.3758/s13428-018-1067-y},
  url = {https://doi.org/10.3758/s13428-018-1067-y},
  urldate = {2024-10-23},
  abstract = {Parameter estimation in evidence-accumulation models of choice response times is demanding of both the data and the user. We outline how to fit evidence-accumulation models using the flexible, open-source, R-based Dynamic Models of Choice (DMC) software. DMC provides a hands-on introduction to the Bayesian implementation of two popular evidence-accumulation models: the diffusion decision model (DDM) and the linear ballistic accumulator (LBA). It enables individual and hierarchical estimation, as well as assessment of the quality of a model’s parameter estimates and descriptive accuracy. First, we introduce the basic concepts of Bayesian parameter estimation, guiding the reader through a simple DDM analysis. We then illustrate the challenges of fitting evidence-accumulation models using a set of LBA analyses. We emphasize best practices in modeling and discuss the importance of parameter- and model-recovery simulations, exploring the strengths and weaknesses of models in different experimental designs and parameter regions. We also demonstrate how DMC can be used to model complex cognitive processes, using as an example a race model of the stop-signal paradigm, which is used to measure inhibitory ability. We illustrate the flexibility of DMC by extending this model to account for mixtures of cognitive processes resulting from attention failures. We then guide the reader through the practical details of a Bayesian hierarchical analysis, from specifying priors to obtaining posterior distributions that encapsulate what has been learned from the data. Finally, we illustrate how the Bayesian approach leads to a quantitatively cumulative science, showing how to use posterior distributions to specify priors that can be used to inform the analysis of future experiments.},
  langid = {english},
  keywords = {Bayesian estimation,Diffusion decison model,Linear ballistic accumulator,Response time,Stop-signal paradigm},
  file = {/Users/rramsey/Dropbox/docs/journals/cog_neuro/Heathcote_2019_Behav ResDynamic models of choiceDynamic models of choice2.pdf},
}
@Article{castro2019,
  title = {Cognitive Workload Measurement and Modeling under Divided Attention},
  author = {Spencer C. Castro and David L. Strayer and Dora Matzke and Andrew Heathcote},
  date = {2019},
  journaltitle = {Journal of Experimental Psychology: Human Perception and Performance},
  volume = {45},
  number = {6},
  pages = {826--839},
  publisher = {American Psychological Association},
  location = {US},
  issn = {1939-1277},
  doi = {10.1037/xhp0000638},
  abstract = {Motorists often engage in secondary tasks unrelated to driving that increase cognitive workload, resulting in fatal crashes and injuries. An International Standards Organization method for measuring a driver’s cognitive workload, the detection response task (DRT), correlates well with driving outcomes, but investigation of its putative theoretical basis in terms of finite attention capacity remains limited. We address this knowledge gap using evidence-accumulation modeling of simple and choice versions of the DRT in a driving scenario. Our experiments demonstrate how dual-task load affects the parameters of evidence-accumulation models. We found that the cognitive workload induced by a secondary task (counting backward by 3s) reduced the rate of evidence accumulation, consistent with rates being sensitive to limited-capacity attention. We also found a compensatory increase in the amount of evidence required for a response and a small speeding in the time for nondecision processes. The International Standards Organization version of the DRT was found to be most sensitive to cognitive workload. A Wald-distributed evidence-accumulation model augmented with a parameter measuring response omissions provided a parsimonious measure of the underlying causes of cognitive workload in this task. This work demonstrates that evidence-accumulation modeling can accurately represent data produced by cognitive workload measurements, reproduce the data through simulation, and provide supporting evidence for the cognitive processes underlying cognitive workload. Our results provide converging evidence that the DRT method is sensitive to dynamic fluctuations in limited-capacity attention. (PsycInfo Database Record (c) 2020 APA, all rights reserved)},
  keywords = {Divided Attention,Drivers,Driving Behavior,Human Channel Capacity,Measurement,Models,Multitasking,Responses,Simulation,Task},
  file = {/Users/rramsey/Dropbox/docs/journals/cog_neuro/Castro_2019_Cognitive workload measurement and modeling under divided attentionCognitive workload measurement and modeling under divided attention.pdf;/Users/rramsey/Zotero/storage/95Q539IB/2019-21152-001.html},
}
@Article{kirschAdditiveRoutesAction2015,
  title = {Additive Routes to Action Learning: Layering Experience Shapes Engagement of the Action Observation Network},
  author = {Louise P Kirsch and Emily S Cross},
  date = {2015},
  journaltitle = {Cerebral Cortex},
  volume = {25},
  number = {12},
  pages = {4799--4811},
  issn = {1460-2199},
}
@Article{parker2023a,
  title = {What Can Evidence Accumulation Modelling Tell Us about Human Social Cognition?},
  author = {Samantha Parker and Richard Ramsey},
  date = {2023-05-08},
  journaltitle = {Quarterly Journal of Experimental Psychology},
  volume = {77},
  number = {3},
  pages = {17470218231176950},
  publisher = {SAGE Publications},
  issn = {1747-0218},
  doi = {10.1177/17470218231176950},
  url = {https://doi.org/10.1177/17470218231176950},
  urldate = {2023-09-25},
  abstract = {Evidence accumulation models are a series of computational models that provide an account for speeded decision-making. These models have been used extensively within the cognitive psychology literature to great success, allowing inferences to be drawn about the psychological processes that underlie cognition that are sometimes not available in a traditional analysis of accuracy or reaction time (RT). Despite this, there have been only a few applications of these models within the domain of social cognition. In this article, we explore several ways in which the study of human social information processing would benefit from application of evidence accumulation modelling. We begin first with a brief overview of the evidence accumulation modelling framework and their past success within the domain of cognitive psychology. We then highlight five ways in which social cognitive research would benefit from an evidence accumulation approach. This includes (1) greater specification of assumptions, (2) unambiguous comparisons across blocked task conditions, (3) quantifying and comparing the magnitude of effects in standardised measures, (4) a novel approach for studying individual differences, and (5) improved reproducibility and accessibility. These points are illustrated using examples from the domain of social attention. Finally, we outline several methodological and practical considerations, which should help researchers use evidence accumulation models productively. Ultimately, it will be seen that evidence accumulation modelling offers a well-developed, accessible, and commonly understood framework that can reveal inferences about cognition that may otherwise be out of reach in a traditional analysis of accuracy and RT. This approach, therefore, has the potential to substantially revise our understanding of social cognition.},
  langid = {english},
  file = {/Users/rramsey/Dropbox/docs/journals/cog_neuro/Parker_Ramsey_2023_What can evidence accumulation modelling tell us about human social cognitionWhat can evidence accumulation modelling tell us about human social cognition.pdf},
}
@InCollection{hintzman1991,
  title = {Why Are Formal Models Useful in Psychology?},
  booktitle = {Relating Theory and Data: {{Essays}} on Human Memory in Honor of {{Bennet B}}. {{Murdock}}.},
  author = {Douglas L. Hintzman},
  date = {1991},
  pages = {39--56},
  publisher = {Lawrence Erlbaum Associates, Inc},
  location = {Hillsdale, NJ, US},
  abstract = {explores the value of formal (mathematical and computer) models in psychology / research on factors that have been shown to bias and limit unaided human reasoning is briefly reviewed, and it is noted that psychologists are susceptible to these errors, in an effort to identify the ways in which models can and cannot aid scientific thought / some limitations of the modeling approach are also discussed the following discussion has four parts / first, I list several sources of error in unaided human reasoning; second, I discuss the nature of formal models; third, I attempt to relate models to reasoning errors, to uncover where the advantages of modeling might lie / finally, I consider the evaluation of formal models, and argue that there are limitations as well as advantages in their use (PsycInfo Database Record (c) 2020 APA, all rights reserved)},
  isbn = {0-8058-0732-2 (Hardcover); 0-8058-0733-0 (Paperback)},
  keywords = {*Computer Applications,*Mathematical Modeling,Reasoning},
}

@Article{yarkoni2022,
  title = {The Generalizability Crisis},
  author = {Tal Yarkoni},
  year = {2022/ed},
  journaltitle = {Behavioral and Brain Sciences},
  volume = {45},
  pages = {e1},
  publisher = {Cambridge University Press},
  issn = {0140-525X, 1469-1825},
  doi = {10.1017/S0140525X20001685},
  url = {https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/abs/generalizability-crisis/AD386115BA539A759ACB3093760F4824},
  urldate = {2023-03-24},
  abstract = {Most theories and hypotheses in psychology are verbal in nature, yet their evaluation overwhelmingly relies on inferential statistical procedures. The validity of the move from qualitative to quantitative analysis depends on the verbal and statistical expressions of a hypothesis being closely aligned – that is, that the two must refer to roughly the same set of hypothetical observations. Here, I argue that many applications of statistical inference in psychology fail to meet this basic condition. Focusing on the most widely used class of model in psychology – the linear mixed model – I explore the consequences of failing to statistically operationalize verbal hypotheses in a way that respects researchers' actual generalization intentions. I demonstrate that although the “random effect” formalism is used pervasively in psychology to model intersubject variability, few researchers accord the same treatment to other variables they clearly intend to generalize over (e.g., stimuli, tasks, or research sites). The under-specification of random effects imposes far stronger constraints on the generalizability of results than most researchers appreciate. Ignoring these constraints can dramatically inflate false-positive rates, and often leads researchers to draw sweeping verbal generalizations that lack a meaningful connection to the statistical quantities they are putatively based on. I argue that failure to take the alignment between verbal and statistical expressions seriously lies at the heart of many of psychology's ongoing problems (e.g., the replication crisis), and conclude with a discussion of several potential avenues for improvement.},
  langid = {english},
  keywords = {Generalization,inference,philosophy of science,psychology,random effects,statistics},
}
@Article{simons2017,
  title = {Constraints on {{Generality}} ({{COG}}): {{A Proposed Addition}} to {{All Empirical Papers}}},
  author = {Daniel J. Simons and Yuichi Shoda and D. Stephen Lindsay},
  date = {2017},
  journaltitle = {Perspectives on Psychological Science},
  volume = {12},
  number = {6},
  pages = {1123--1128},
  doi = {10.1177/1745691617708630},
  url = {http://journals.sagepub.com/doi/abs/10.1177/1745691617708630},
  abstract = {Psychological scientists draw inferences about populations based on samples—of people, situations, and stimuli—from those populations. Yet, few papers identify their target populations, and even fewer justify how or why the tested samples are representative of broader populations. A cumulative science depends on accurately characterizing the generality of findings, but current publishing standards do not require authors to constrain their inferences, leaving readers to assume the broadest possible generalizations. We propose that the discussion section of all primary research articles specify Constraints on Generality (i.e., a “COG” statement) that identify and justify target populations for the reported findings. Explicitly defining the target populations will help other researchers to sample from the same populations when conducting a direct replication, and it could encourage follow-up studies that test the boundary conditions of the original finding. Universal adoption of COG statements would change publishing incentives to favor a more cumulative science.},
  keywords = {generalizability,meta-science,open science,replication,reproducibility,science communication,transparency},
}
@Article{ratcliff2004,
  title = {A {{Diffusion Model Analysis}} of the {{Effects}} of {{Aging}} in the {{Lexical-Decision Task}}},
  author = {Roger Ratcliff and Anjali Thapar and Pablo Gomez and Gail McKoon},
  date = {2004},
  journaltitle = {Psychology and Aging},
  volume = {19},
  number = {2},
  pages = {278--289},
  publisher = {American Psychological Association},
  location = {US},
  issn = {1939-1498},
  doi = {10.1037/0882-7974.19.2.278},
  abstract = {The effects of aging on response time (RT) are examined in 2 lexical-decision experiments with young and older subjects (age 60-75). The results show that the older subjects were slower than the young subjects, but more accurate. R. Ratcliff s (1978) diffusion model provided a good account of RTs, their distributions, and response accuracy. The fits show an 80-100-ms slowing of the nondecision components of RT for older subjects relative to young subjects and more conservative decision criterion settings for older subjects than for young subjects. The rates of accumulation of evidence were not significantly different for older compared with young subjects (less than 2\% and 5\% higher for older subjects relative to young subjects in the 2 experiments). (PsycInfo Database Record (c) 2023 APA, all rights reserved)},
  keywords = {Age Differences,Aging,Lexical Decision,Models,Reaction Time},
  file = {/Users/rramsey/Dropbox/docs/journals/cog_neuro/Ratcliff_2004_A Diffusion Model Analysis of the Effects of Aging in the Lexical-Decision TaskA Diffusion Model Analysis of the Effects of Aging in the Lexical-Decision Task.pdf;/Users/rramsey/Zotero/storage/GQ5H7F6X/2004-14948-004.html},
}
@Manual{R-papaja,
  type = {manual},
  title = {{{papaja}}: {{Prepare}} Reproducible {{APA}} Journal Articles with {{R Markdown}}},
  author = {Frederik Aust and Marius Barth},
  date = {2023},
  url = {https://github.com/crsh/papaja},
}
